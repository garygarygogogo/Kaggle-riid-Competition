{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-13T03:27:48.403105Z",
     "iopub.status.busy": "2020-12-13T03:27:48.402244Z",
     "iopub.status.idle": "2020-12-13T03:27:49.675494Z",
     "shell.execute_reply": "2020-12-13T03:27:49.676096Z"
    },
    "papermill": {
     "duration": 1.284212,
     "end_time": "2020-12-13T03:27:49.676309",
     "exception": false,
     "start_time": "2020-12-13T03:27:48.392097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc #垃圾回收\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict #defaultdict的作用是在于，当字典的key不存在但被查找时，返回的不是keyError而是一个默认值-1\n",
    "from tqdm.notebook import tqdm #进度条\n",
    "import lightgbm as lgb\n",
    "# import riiideducation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os#文件系统接入模块\n",
    "\n",
    "from joblib import Parallel, delayed#多线程并行计算\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import bisect#二分查找\n",
    "from utils import dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVAE_NAME = 'lgb.seed42.200.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_and_lecture_parsing():\n",
    "    q = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv').fillna('')#空白字符串填充\n",
    "    bundle_dict = q.groupby('bundle_id')['question_id'].count().to_dict()#groupby就是按照bundle_id分组排序，然后统计数量\n",
    "    q.loc[:, 'small_part'] = -1#全部赋值为-1，初始值\n",
    "    cnt = 0\n",
    "    for part in range(1, 8):#讲q上面打包的id值全部打包存成元组形式\n",
    "        end = q[q.part == part][q[q.part == part]['question_id'].diff(-1) != -1]['question_id'].values\n",
    "        \n",
    "        start = q[q.part == part][q[q.part == part]['question_id'].diff() != 1]['question_id'].values\n",
    "        for s,t in zip(start, end):#\n",
    "            q.loc[s:t, 'small_part'] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "    q['part'] -= 1 \n",
    "    \n",
    "    for i in range(3):\n",
    "        #原始数据tags变成3列，变成列表，没有的话就变成一个空值\n",
    "        q[f'tags{i}'] = q['tags'].apply(lambda x: [int(item) for item in x.split()][i] if len(x.split()) > i else np.nan)\n",
    "    for i in range(1,3):\n",
    "        q[f'tags{i}'] = q[f'tags{i-1}']*200+q[f'tags{i}'] #选了200组\n",
    "        \n",
    "    q['tags_count'] = q['tags'].apply(lambda x: len(x.split())) \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdiv(a, b):#归一化处理\n",
    "    if b == 0:\n",
    "        return np.nan\n",
    "    return a*1./b\n",
    "\n",
    "def ll(predicted, actual, eps=1e-14):#归一化以及损失函数\n",
    "    predicted = np.clip(predicted, eps, 1-eps)#大于最大值的全部变成最大值，小于最小值的全部变成最小值\n",
    "    loss = -1*(actual * np.log(predicted) + (1 - actual) * np.log(1-predicted))#交叉熵损失函数\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parallel(results):#二维的数据\n",
    "    dfs = [item[0] for item in results]\n",
    "    dicts = [item[1] for item in results]\n",
    "    train_data = pd.concat(dfs, axis=0)#按照行合并\n",
    "    df_train = train_data[(train_data.tag==1)]#按照列合并\n",
    "    df_valid = train_data[(train_data.tag==0)]#验证集按照行合并\n",
    "    \n",
    "    feature_dicts = dicts[0]#特征字典\n",
    "    \n",
    "    for item in dicts[1:]:\n",
    "        for feature_name in item.keys():#提取特征，并遍历\n",
    "            feature_dicts[feature_name].update(item[feature_name])#dict.update(dict2)后面的用来升级\n",
    "    \n",
    "    return df_train, df_valid, feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-13T03:27:49.693474Z",
     "iopub.status.busy": "2020-12-13T03:27:49.691859Z",
     "iopub.status.idle": "2020-12-13T04:07:01.519985Z",
     "shell.execute_reply": "2020-12-13T04:07:01.521116Z"
    },
    "papermill": {
     "duration": 2351.840337,
     "end_time": "2020-12-13T04:07:01.521775",
     "exception": false,
     "start_time": "2020-12-13T03:27:49.681438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 随机种子\n",
    "SEED = 42\n",
    "\n",
    "# 随机种子\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    \n",
    "seed_everything(SEED)\n",
    "\n",
    "# Funcion for user stats with loops\n",
    "def make_data(df, global_avg_q_time_dict, prior_question_elapsed_time_mean, global_content_cnt_dict,update = True):\n",
    "#     # 全部转换成浮点型\n",
    "    last_u_content_id_dict = defaultdict(float)\n",
    "    last_u_container_id_dict = defaultdict(float)\n",
    "    hist_u_answered_correctly_cnt_dict = defaultdict(float)\n",
    "    hist_u_elapsed_time_sum_dict = defaultdict(float)\n",
    "    hist_u_explanation_sum_dict = defaultdict(float)\n",
    "    hist_u_same_part_correctly_cnt_dict = defaultdict(dd)\n",
    "    hist_u_same_content_id_correctly_cnt_dict = defaultdict(dd)\n",
    "    timestamp_u = defaultdict(list)\n",
    "    hist_u_answered_correctly_sum_dict = defaultdict(float)\n",
    "    hist_u_score_sum_dict = defaultdict(float)\n",
    "    hist_u_same_part_correctly_sum_dict = defaultdict(dd)\n",
    "    hist_u_same_content_id_correctly_sum_dict = defaultdict(dd)\n",
    "    last_u_last_incorrect_timestamp_dict = defaultdict(float)\n",
    "    hist_u_last_incorrect_cnt_dict = defaultdict(float)\n",
    "    hist_u_lag_time_sum_dict = defaultdict(float)\n",
    "    \n",
    "    # -----------------------------------创建零矩阵，后期将字典中的数据放到这里面\n",
    "    last_u_diff_container_id = np.zeros(len(df), dtype = np.float32)\n",
    "    timestamp_u_recency_1 = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_gap_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_avg_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_lag_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_lag_time = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    hist_u_lag_time_raito = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_answered_correctly_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_elapsed_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_explanation_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_score_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_same_part_sum = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_part_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_part_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_sum = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_last_incorrect_timestamp = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_last_incorrect_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    # User Question\n",
    "    answered_correctly_uq_count = np.zeros(len(df), dtype = np.int32)\n",
    "    # -----------------------------------------------------------------------\n",
    "    \n",
    "    #从数组中提取特征\n",
    "    for num, row in tqdm(enumerate(df[['user_id','answered_correctly','content_id','prior_question_elapsed_time', \n",
    "                                  'prior_question_had_explanation', 'timestamp','contentid_mean','task_container_id','part']].values)):\n",
    "                \n",
    "        last_u_content_id = last_u_content_id_dict.get(row[0], np.nan)#get（）函数返回空值的位置\n",
    "        last_u_container_id = last_u_container_id_dict.get(row[0], np.nan)\n",
    "        \n",
    "        last_u_diff_container_id[num] = row[7] - last_u_container_id   # 去除空值\n",
    "\n",
    "        last_u_sum_time = row[3] * global_content_cnt_dict.get(last_u_content_id, 1)#用户的id做标记\n",
    "        #每个内容乘平均时间=总时间\n",
    "        sum_time_consum = global_avg_q_time_dict.get(row[2], prior_question_elapsed_time_mean) * global_content_cnt_dict.get(row[2], 1)\n",
    "        \n",
    "        # Client features assignation\n",
    "        # ------------------------------------------------------------------\n",
    "        #如果回答的时间为0\n",
    "        if len(timestamp_u[row[0]]) == 0:\n",
    "            timestamp_u_recency_1[num] = np.nan     # 2\n",
    "            timestamp_u_gap_time_ratio[num] = np.nan  # 3\n",
    "            timestamp_u_avg_time_ratio[num] = np.nan  # 4\n",
    "            timestamp_u_lag_time_ratio[num] = np.nan  # 5\n",
    "            timestamp_u_lag_time[num]  = np.nan # 6\n",
    "        #回答时间为1，然后做比例归一化处理\n",
    "        elif len(timestamp_u[row[0]]) == 1:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][0]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][0]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][0]), global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = np.nan\n",
    "            timestamp_u_lag_time[num]  = np.nan\n",
    "            \n",
    "        elif len(timestamp_u[row[0]]) == 2:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][1]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][1]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][1]) , global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = mdiv((timestamp_u[row[0]][1] - timestamp_u[row[0]][0] - last_u_sum_time) , (row[5] - timestamp_u[row[0]][1]+1))\n",
    "            timestamp_u_lag_time[num] = (timestamp_u[row[0]][1] - timestamp_u[row[0]][0] - last_u_sum_time)\n",
    "            \n",
    "        elif len(timestamp_u[row[0]]) == 3:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][2]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][2]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][2]) , global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = mdiv((timestamp_u[row[0]][2] - timestamp_u[row[0]][1] - sum_time_consum) , (row[5] - timestamp_u[row[0]][2]+1))\n",
    "            timestamp_u_lag_time[num] = (timestamp_u[row[0]][2] - timestamp_u[row[0]][1] - last_u_sum_time)\n",
    "        #如果时间不是空值，还是做归一化处理\n",
    "        if timestamp_u_lag_time[num] is not np.nan:    \n",
    "            hist_u_lag_time_sum_dict[row[0]] += max(0, min(timestamp_u_lag_time[num], 300*1000))\n",
    "        hist_u_lag_time_raito[num] = mdiv(hist_u_lag_time_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "\n",
    "                \n",
    "        hist_u_answered_correctly_ratio[num] = mdiv(hist_u_answered_correctly_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        hist_u_elapsed_time_ratio[num] = mdiv(hist_u_elapsed_time_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        hist_u_explanation_ratio[num] = mdiv(hist_u_explanation_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "            \n",
    "        hist_u_score_ratio[num] = mdiv(hist_u_score_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        \n",
    "        hist_u_same_part_sum[num] = hist_u_same_part_correctly_sum_dict[row[0]][row[8]]\n",
    "        hist_u_same_part_cnt[num] = hist_u_same_part_correctly_cnt_dict[row[0]][row[8]]\n",
    "        hist_u_same_part_ratio[num] = mdiv(hist_u_same_part_sum[num], hist_u_same_part_cnt[num])\n",
    "\n",
    "        hist_u_same_content_id_sum[num] = hist_u_same_content_id_correctly_sum_dict[row[0]][row[2]]\n",
    "        hist_u_same_content_id_cnt[num] = hist_u_same_content_id_correctly_cnt_dict[row[0]][row[2]]        \n",
    "        hist_u_same_content_id_ratio[num] = mdiv(hist_u_same_content_id_sum[num], hist_u_same_content_id_cnt[num])\n",
    "    \n",
    "        hist_u_last_incorrect_timestamp[num] = row[5] - last_u_last_incorrect_timestamp_dict[row[0]]\n",
    "        hist_u_last_incorrect_cnt[num] = hist_u_last_incorrect_cnt_dict[row[0]]\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # Client features updates 特征更新原来row[0]是空值\n",
    "        hist_u_answered_correctly_cnt_dict[row[0]] += 1\n",
    "        hist_u_elapsed_time_sum_dict[row[0]] += row[3]\n",
    "        hist_u_explanation_sum_dict[row[0]] += int(row[4])\n",
    "        hist_u_same_part_correctly_cnt_dict[row[0]][row[8]] +=1\n",
    "        hist_u_same_content_id_correctly_cnt_dict[row[0]][row[2]] += 1\n",
    "      #row[2]和row[7]一个是回答成功一个是回答不成功\n",
    "        if len(timestamp_u[row[0]])==0 or row[5]!=timestamp_u[row[0]][-1]:\n",
    "            last_u_content_id_dict[row[0]] = row[2]\n",
    "            if len(timestamp_u[row[0]]) == 3:\n",
    "                timestamp_u[row[0]].pop(0)\n",
    "                timestamp_u[row[0]].append(row[5])\n",
    "            else:\n",
    "                timestamp_u[row[0]].append(row[5])\n",
    "        if len(timestamp_u[row[0]])!=0 or row[5]==timestamp_u[row[0]][-1]:\n",
    "            last_u_container_id_dict[row[0]] = row[7]\n",
    "        \n",
    "        # Flag for training and inference\n",
    "        if update:\n",
    "            # ------------------------------------------------------------------\n",
    "            # Client features updates\n",
    "            hist_u_answered_correctly_sum_dict[row[0]] += row[1]\n",
    "            hist_u_score_sum_dict[row[0]] += (1 if row[1] == 1 else -1) * ll(row[6], row[1])\n",
    "            hist_u_same_part_correctly_sum_dict[row[0]][row[8]] += row[1]\n",
    "            hist_u_same_content_id_correctly_sum_dict[row[0]][row[2]] += row[1]\n",
    "            if row[1] == 0:\n",
    "                last_u_last_incorrect_timestamp_dict[row[0]] = row[5]\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] = 0\n",
    "            else:\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] += 1\n",
    "    #字典：标签：名字\n",
    "    user_df = pd.DataFrame({\n",
    "                            'last_u_diff_container_id': last_u_diff_container_id, \n",
    "                            'timestamp_u_recency_1': timestamp_u_recency_1, \n",
    "                            'timestamp_u_gap_time_ratio': timestamp_u_gap_time_ratio, \n",
    "                            'timestamp_u_avg_time_ratio': timestamp_u_avg_time_ratio, \n",
    "                            'timestamp_u_lag_time_ratio': timestamp_u_lag_time_ratio, \n",
    "                            'timestamp_u_lag_time': timestamp_u_lag_time, \n",
    "        \n",
    "                            'hist_u_lag_time_raito': hist_u_lag_time_raito,\n",
    "                            'hist_u_answered_correctly_ratio': hist_u_answered_correctly_ratio, \n",
    "                            'hist_u_elapsed_time_ratio': hist_u_elapsed_time_ratio, \n",
    "                            'hist_u_explanation_ratio': hist_u_explanation_ratio,\n",
    "                            'hist_u_score_ratio': hist_u_score_ratio, \n",
    "                            'hist_u_same_part_sum': hist_u_same_part_sum,\n",
    "                            'hist_u_same_part_cnt': hist_u_same_part_cnt,\n",
    "                            'hist_u_same_part_ratio': hist_u_same_part_ratio,\n",
    "                            'hist_u_same_content_id_sum': hist_u_same_content_id_sum,\n",
    "                            'hist_u_same_content_id_cnt':hist_u_same_content_id_cnt,\n",
    "                            'hist_u_same_content_id_ratio':hist_u_same_content_id_ratio,\n",
    "                            'hist_u_last_incorrect_timestamp':hist_u_last_incorrect_timestamp,\n",
    "                            'hist_u_last_incorrect_cnt':hist_u_last_incorrect_cnt,\n",
    "\n",
    "                            'timestamp': df['timestamp'].values,\n",
    "                            'content_id': df['content_id'].values,\n",
    "                            'task_container_id': df['task_container_id'].values,\n",
    "                            'prior_question_elapsed_time': df['prior_question_elapsed_time'].values,\n",
    "                            'part': df['part'].values,\n",
    "                            'tags0': df['tags0'].values,\n",
    "                            'tags1': df['tags1'].values,\n",
    "                            'tags2': df['tags2'].values,\n",
    "                            'tags_count': df['tags_count'].values,                 \n",
    "                            'contentid_mean': df['contentid_mean'].values,\n",
    "                            'tag':df['tag'].values,\n",
    "                            'answered_correctly': df['answered_correctly'].values,\n",
    "                           })\n",
    "    \n",
    "    \n",
    "    features_dicts = {\n",
    "        'last_u_content_id_dict':last_u_content_id_dict,\n",
    "        'last_u_container_id_dict':last_u_container_id_dict,\n",
    "        'hist_u_answered_correctly_cnt_dict':hist_u_answered_correctly_cnt_dict,\n",
    "        'hist_u_elapsed_time_sum_dict':hist_u_elapsed_time_sum_dict,\n",
    "        'hist_u_explanation_sum_dict':hist_u_explanation_sum_dict,\n",
    "        'hist_u_same_part_correctly_cnt_dict':hist_u_same_part_correctly_cnt_dict,\n",
    "        'hist_u_same_content_id_correctly_cnt_dict':hist_u_same_content_id_correctly_cnt_dict,\n",
    "        'timestamp_u':timestamp_u,\n",
    "        'hist_u_answered_correctly_sum_dict':hist_u_answered_correctly_sum_dict,\n",
    "        'hist_u_score_sum_dict':hist_u_score_sum_dict,\n",
    "        'hist_u_same_part_correctly_sum_dict':hist_u_same_part_correctly_sum_dict,\n",
    "        'hist_u_same_content_id_correctly_sum_dict':hist_u_same_content_id_correctly_sum_dict,\n",
    "        'last_u_last_incorrect_timestamp_dict':last_u_last_incorrect_timestamp_dict,\n",
    "        'hist_u_last_incorrect_cnt_dict':hist_u_last_incorrect_cnt_dict,\n",
    "        'hist_u_lag_time_sum_dict':hist_u_lag_time_sum_dict,\n",
    "    }    \n",
    "    \n",
    "    return user_df, features_dicts\n",
    "\n",
    "# Funcion for user stats with loops\n",
    "def add_features(df, \n",
    "        last_u_content_id_dict,\n",
    "        last_u_container_id_dict,\n",
    "        hist_u_answered_correctly_cnt_dict,\n",
    "        hist_u_elapsed_time_sum_dict,\n",
    "        hist_u_explanation_sum_dict,\n",
    "        hist_u_same_part_correctly_cnt_dict,\n",
    "        hist_u_same_content_id_correctly_cnt_dict,\n",
    "        timestamp_u,\n",
    "        hist_u_answered_correctly_sum_dict,\n",
    "        hist_u_score_sum_dict,\n",
    "        hist_u_same_part_correctly_sum_dict,\n",
    "        hist_u_same_content_id_correctly_sum_dict,\n",
    "        last_u_last_incorrect_timestamp_dict,\n",
    "        hist_u_last_incorrect_cnt_dict,\n",
    "        hist_u_lag_time_sum_dict,\n",
    "        global_avg_q_time_dict, prior_question_elapsed_time_mean, global_content_cnt_dict,\n",
    "        update = True):\n",
    "  \n",
    "    # -----------------------------------------------------------------------\n",
    "    last_u_diff_container_id = np.zeros(len(df), dtype = np.float32)\n",
    "    timestamp_u_recency_1 = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_gap_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_avg_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_lag_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    timestamp_u_lag_time = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    hist_u_lag_time_raito = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_answered_correctly_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_elapsed_time_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_explanation_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_score_ratio = np.zeros(len(df), dtype = np.float32) \n",
    "    hist_u_same_part_sum = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_part_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_part_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_sum = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_same_content_id_ratio = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_last_incorrect_timestamp = np.zeros(len(df), dtype = np.float32)\n",
    "    hist_u_last_incorrect_cnt = np.zeros(len(df), dtype = np.float32)\n",
    "    # User Question\n",
    "    answered_correctly_uq_count = np.zeros(len(df), dtype = np.int32)\n",
    "    # -----------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    for num, row in tqdm(enumerate(df[['user_id','answered_correctly','content_id','prior_question_elapsed_time', \n",
    "                                  'prior_question_had_explanation', 'timestamp','contentid_mean','task_container_id','part']].values)):\n",
    "                \n",
    "        last_u_content_id = last_u_content_id_dict.get(row[0], np.nan)\n",
    "        last_u_container_id = last_u_container_id_dict.get(row[0], np.nan)\n",
    "        \n",
    "        last_u_diff_container_id[num] = row[7] - last_u_container_id   # 1\n",
    "\n",
    "        last_u_sum_time = row[3] * global_content_cnt_dict.get(last_u_content_id, 1)\n",
    "        sum_time_consum = global_avg_q_time_dict.get(row[2], prior_question_elapsed_time_mean) * global_content_cnt_dict.get(row[2], 1)\n",
    "        \n",
    "        # Client features assignation\n",
    "        # ------------------------------------------------------------------\n",
    "            \n",
    "        if len(timestamp_u[row[0]]) == 0:\n",
    "            timestamp_u_recency_1[num] = np.nan     # 2\n",
    "            timestamp_u_gap_time_ratio[num] = np.nan  # 3\n",
    "            timestamp_u_avg_time_ratio[num] = np.nan  # 4\n",
    "            timestamp_u_lag_time_ratio[num] = np.nan  # 5\n",
    "            timestamp_u_lag_time[num]  = np.nan # 6\n",
    "            \n",
    "        elif len(timestamp_u[row[0]]) == 1:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][0]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][0]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][0]), global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = np.nan\n",
    "            timestamp_u_lag_time[num]  = np.nan\n",
    "            \n",
    "        elif len(timestamp_u[row[0]]) == 2:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][1]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][1]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][1]) , global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = mdiv((timestamp_u[row[0]][1] - timestamp_u[row[0]][0] - last_u_sum_time) , (row[5] - timestamp_u[row[0]][1]+1))\n",
    "            timestamp_u_lag_time[num] = (timestamp_u[row[0]][1] - timestamp_u[row[0]][0] - last_u_sum_time)\n",
    "            \n",
    "        elif len(timestamp_u[row[0]]) == 3:\n",
    "            timestamp_u_recency_1[num] = row[5] - timestamp_u[row[0]][2]\n",
    "            timestamp_u_gap_time_ratio[num] = mdiv(sum_time_consum , (row[5] - timestamp_u[row[0]][2]))\n",
    "            timestamp_u_avg_time_ratio[num] = mdiv((row[5] - timestamp_u[row[0]][2]) , global_content_cnt_dict.get(row[2], 1))\n",
    "            timestamp_u_lag_time_ratio[num] = mdiv((timestamp_u[row[0]][2] - timestamp_u[row[0]][1] - sum_time_consum) , (row[5] - timestamp_u[row[0]][2]+1))\n",
    "            timestamp_u_lag_time[num] = (timestamp_u[row[0]][2] - timestamp_u[row[0]][1] - last_u_sum_time)\n",
    "\n",
    "        if timestamp_u_lag_time[num] is not np.nan:    \n",
    "            hist_u_lag_time_sum_dict[row[0]] += max(0, min(timestamp_u_lag_time[num], 300*1000))\n",
    "        hist_u_lag_time_raito[num] = mdiv(hist_u_lag_time_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "\n",
    "                \n",
    "        hist_u_answered_correctly_ratio[num] = mdiv(hist_u_answered_correctly_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        hist_u_elapsed_time_ratio[num] = mdiv(hist_u_elapsed_time_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        hist_u_explanation_ratio[num] = mdiv(hist_u_explanation_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "            \n",
    "        hist_u_score_ratio[num] = mdiv(hist_u_score_sum_dict[row[0]], hist_u_answered_correctly_cnt_dict[row[0]])\n",
    "        \n",
    "        hist_u_same_part_sum[num] = hist_u_same_part_correctly_sum_dict[row[0]][row[8]]\n",
    "        hist_u_same_part_cnt[num] = hist_u_same_part_correctly_cnt_dict[row[0]][row[8]]\n",
    "        hist_u_same_part_ratio[num] = mdiv(hist_u_same_part_sum[num], hist_u_same_part_cnt[num])\n",
    "\n",
    "        hist_u_same_content_id_sum[num] = hist_u_same_content_id_correctly_sum_dict[row[0]][row[2]]\n",
    "        hist_u_same_content_id_cnt[num] = hist_u_same_content_id_correctly_cnt_dict[row[0]][row[2]]        \n",
    "        hist_u_same_content_id_ratio[num] = mdiv(hist_u_same_content_id_sum[num], hist_u_same_content_id_cnt[num])\n",
    "    \n",
    "        hist_u_last_incorrect_timestamp[num] = row[5] - last_u_last_incorrect_timestamp_dict[row[0]]\n",
    "        hist_u_last_incorrect_cnt[num] = hist_u_last_incorrect_cnt_dict[row[0]]\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # Client features updates\n",
    "        hist_u_answered_correctly_cnt_dict[row[0]] += 1\n",
    "        hist_u_elapsed_time_sum_dict[row[0]] += row[3]\n",
    "        hist_u_explanation_sum_dict[row[0]] += int(row[4])\n",
    "        hist_u_same_part_correctly_cnt_dict[row[0]][row[8]] +=1\n",
    "        hist_u_same_content_id_correctly_cnt_dict[row[0]][row[2]] += 1\n",
    "\n",
    "        if len(timestamp_u[row[0]])==0 or row[5]!=timestamp_u[row[0]][-1]:\n",
    "            last_u_content_id_dict[row[0]] = row[2]\n",
    "            if len(timestamp_u[row[0]]) == 3:\n",
    "                timestamp_u[row[0]].pop(0)\n",
    "                timestamp_u[row[0]].append(row[5])\n",
    "            else:\n",
    "                timestamp_u[row[0]].append(row[5])\n",
    "        if len(timestamp_u[row[0]])!=0 or row[5]==timestamp_u[row[0]][-1]:\n",
    "            last_u_container_id_dict[row[0]] = row[7]\n",
    "        \n",
    "        # Flag for training and inference\n",
    "        if update:\n",
    "            # ------------------------------------------------------------------\n",
    "            # Client features updates\n",
    "            hist_u_answered_correctly_sum_dict[row[0]] += row[1]\n",
    "            hist_u_score_sum_dict[row[0]] += (1 if row[1] == 1 else -1) * ll(row[6], row[1])\n",
    "            hist_u_same_part_correctly_sum_dict[row[0]][row[8]] += row[1]\n",
    "            hist_u_same_content_id_correctly_sum_dict[row[0]][row[2]] += row[1]\n",
    "            if row[1] == 0:\n",
    "                last_u_last_incorrect_timestamp_dict[row[0]] = row[5]\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] = 0\n",
    "            else:\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] += 1\n",
    "\n",
    "    user_df = pd.DataFrame({\n",
    "                            'last_u_diff_container_id': last_u_diff_container_id, \n",
    "                            'timestamp_u_recency_1': timestamp_u_recency_1, \n",
    "                            'timestamp_u_gap_time_ratio': timestamp_u_gap_time_ratio, \n",
    "                            'timestamp_u_avg_time_ratio': timestamp_u_avg_time_ratio, \n",
    "                            'timestamp_u_lag_time_ratio': timestamp_u_lag_time_ratio, \n",
    "                            'timestamp_u_lag_time': timestamp_u_lag_time, \n",
    "        \n",
    "                            'hist_u_lag_time_raito': hist_u_lag_time_raito,\n",
    "                            'hist_u_answered_correctly_ratio': hist_u_answered_correctly_ratio, \n",
    "                            'hist_u_elapsed_time_ratio': hist_u_elapsed_time_ratio, \n",
    "                            'hist_u_explanation_ratio': hist_u_explanation_ratio,\n",
    "                            'hist_u_score_ratio': hist_u_score_ratio, \n",
    "                            'hist_u_same_part_sum': hist_u_same_part_sum,\n",
    "                            'hist_u_same_part_cnt': hist_u_same_part_cnt,\n",
    "                            'hist_u_same_part_ratio': hist_u_same_part_ratio,\n",
    "                            'hist_u_same_content_id_sum': hist_u_same_content_id_sum,\n",
    "                            'hist_u_same_content_id_cnt':hist_u_same_content_id_cnt,\n",
    "                            'hist_u_same_content_id_ratio':hist_u_same_content_id_ratio,\n",
    "                            'hist_u_last_incorrect_timestamp':hist_u_last_incorrect_timestamp,\n",
    "                            'hist_u_last_incorrect_cnt':hist_u_last_incorrect_cnt,        \n",
    "                           })\n",
    "    \n",
    "    df = pd.concat([df.reset_index(drop=True), user_df], axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_features(df, \n",
    "                    hist_u_answered_correctly_sum_dict, \n",
    "                    hist_u_score_sum_dict, \n",
    "                    hist_u_same_part_correctly_sum_dict, \n",
    "                    hist_u_same_content_id_correctly_sum_dict,\n",
    "                    last_u_last_incorrect_timestamp_dict,\n",
    "                    hist_u_last_incorrect_cnt_dict):\n",
    "    for row in df[['user_id','answered_correctly','content_id','prior_question_elapsed_time', \n",
    "                    'prior_question_had_explanation', 'timestamp','contentid_mean',\n",
    "                   'task_container_id','part','content_type_id']].values:\n",
    "        if row[-1] == 0:\n",
    "            # ------------------------------------------------------------------\n",
    "            # Client features updates\n",
    "            hist_u_answered_correctly_sum_dict[row[0]] += row[1]\n",
    "            hist_u_score_sum_dict[row[0]] += (1 if row[1] == 1 else -1) * ll(row[6], row[1])\n",
    "            hist_u_same_part_correctly_sum_dict[row[0]][row[8]] += row[1]\n",
    "            hist_u_same_content_id_correctly_sum_dict[row[0]][row[2]] += row[1]\n",
    "            if row[1] == 0:\n",
    "                last_u_last_incorrect_timestamp_dict[row[0]] = row[5]\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] = 0\n",
    "            else:\n",
    "                hist_u_last_incorrect_cnt_dict[row[0]] += 1\n",
    "            \n",
    "    return\n",
    "\n",
    "def read_and_preprocess(feature_engineering = False):\n",
    "    nrows = None\n",
    "    #所有标签\n",
    "    tags = '../input/folds/tag_data_full.pkl'\n",
    "    question_file = '../input/riiid-test-answer-prediction/questions.csv'\n",
    "\n",
    "    train_data = pd.read_csv('../input/riiid-test-answer-prediction/train.csv', \n",
    "        nrows=None, \n",
    "        dtype={\n",
    "            'row_id': 'int64', \n",
    "            'timestamp': 'int64', \n",
    "            'user_id': 'int32', \n",
    "            'content_id': 'int16', \n",
    "            'content_type_id': 'int8',\n",
    "            'task_container_id': 'int16', \n",
    "            'user_answer': 'int8', \n",
    "            'answered_correctly': 'int8', \n",
    "            'prior_question_elapsed_time': 'float32', \n",
    "            'prior_question_had_explanation': 'boolean'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tags = pd.read_pickle(tags)\n",
    "    train_data.loc[:,'tag'] = tags\n",
    "    \n",
    "    # Read data\n",
    "    feld_needed = ['timestamp', 'user_id', 'answered_correctly', 'content_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation','task_container_id','tag']\n",
    "    train = train_data[(train_data.tag==1)][feld_needed]\n",
    "    valid = train_data[(train_data.tag==0)][feld_needed]\n",
    "    \n",
    "    # Filter by content_type_id to discard lectures\n",
    "    train = train.loc[train.content_type_id == False].reset_index(drop = True)#重置对应索引\n",
    "    valid = valid.loc[valid.content_type_id == False].reset_index(drop = True)\n",
    "    \n",
    "    # Changing dtype to avoid lightgbm error\n",
    "    train['prior_question_had_explanation'] = train.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "    valid['prior_question_had_explanation'] = valid.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "    \n",
    "    # Fill prior question elapsed time with the mean\n",
    "    #空值填充成平均值\n",
    "    prior_question_elapsed_time_mean = train['prior_question_elapsed_time'].dropna().mean()\n",
    "    train['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    valid['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    \n",
    "    # Merge with question dataframe\n",
    "    questions_df = questions_and_lecture_parsing()\n",
    "    questions_df['part'] = questions_df['part'].astype(np.int32)\n",
    "    questions_df['bundle_id'] = questions_df['bundle_id'].astype(np.int32)\n",
    "    \n",
    "    global_content_cnt_dict = questions_df.bundle_id.value_counts().to_dict()\n",
    "    #合并相同项\n",
    "    train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    valid = pd.merge(valid, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    \n",
    "    ##### global target encoding features ######\n",
    "    contentid_count = train['content_id'].value_counts()\n",
    "    \n",
    "    contentidmean_dict = train.groupby('content_id')['answered_correctly'].mean()\n",
    "    contentidmean_dict = contentidmean_dict[contentid_count>10].to_dict()\n",
    "    train['contentid_mean'] = train['content_id'].map(lambda x:contentidmean_dict.get(x,np.nan))\n",
    "    valid['contentid_mean'] = valid['content_id'].map(lambda x:contentidmean_dict.get(x,np.nan))\n",
    "    \n",
    "    #########\n",
    "\n",
    "    prior_question_elapsed_time_mean = train['prior_question_elapsed_time'].dropna().mean()\n",
    "    \n",
    "    #### global avg time\n",
    "    ret = defaultdict(list)\n",
    "    question_data = train_data[train_data.content_type_id==0].copy()\n",
    "    question_data = question_data[question_data.timestamp.diff()!=0]\n",
    "    elapsed_time = question_data.prior_question_elapsed_time.shift(-1).values\n",
    "    bundle_id = question_data.content_id.astype(int).values\n",
    "#数据归一化处理\n",
    "    for x, y in zip(bundle_id, elapsed_time):\n",
    "        if not np.isnan(y): \n",
    "            if not x in ret: ret[x] = []\n",
    "            ret[x].append(y)   \n",
    "    global_avg_q_time_dict = {}\n",
    "    for x,y in ret.items():\n",
    "        if len(y) > 100:\n",
    "            y = sorted(y)\n",
    "            global_avg_q_time_dict[x] = np.mean(y)\n",
    "    ####\n",
    "    # Fill prior question elapsed time with the mean\n",
    "    train['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    valid['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    \n",
    "\n",
    "    print('User feature calculation started...')\n",
    "    print('\\n')\n",
    "    train_data = pd.concat([train, valid], axis=0).reset_index(drop = True)\n",
    "    user_id = train_data['user_id'].unique()\n",
    "    \n",
    "    n_parts = 4\n",
    "    n_cores= 20\n",
    "    period = len(user_id) // (n_cores * n_parts - 1)\n",
    "    userid_groups = [user_id[i*period:(i+1)*period] for i in range(n_cores * n_parts)] \n",
    "    \n",
    "    results_all = []\n",
    "    def my_make_data(df):\n",
    "        return make_data(df, global_avg_q_time_dict, prior_question_elapsed_time_mean, global_content_cnt_dict)\n",
    "    #并行计算\n",
    "    for i in range(n_parts):\n",
    "        print('PART {}'.format(i))\n",
    "        split_dfs = [train_data[train_data.user_id.isin(ids)] for ids in userid_groups[i*n_cores:(i+1)*n_cores]]\n",
    "        print([item.shape for item in split_dfs])\n",
    "        results = Parallel(n_jobs=n_cores)(delayed(my_make_data)(item) for item in split_dfs)\n",
    "        results_all.extend(results)\n",
    "\n",
    "    df_train, df_valid, features_dicts = merge_parallel(results_all)\n",
    "                                        \n",
    "    gc.collect()\n",
    "    \n",
    "    print('User feature calculation completed...')\n",
    "    print('\\n')\n",
    "    global_dicts = {\n",
    "        'global_avg_q_time_dict': global_avg_q_time_dict,\n",
    "        'prior_question_elapsed_time_mean':prior_question_elapsed_time_mean,\n",
    "        'global_content_cnt_dict':global_content_cnt_dict,\n",
    "        'contentidmean_dict':contentidmean_dict\n",
    "    }\n",
    "    \n",
    "    return df_train, df_valid, questions_df, prior_question_elapsed_time_mean, features_dicts, global_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-13T03:27:49.693474Z",
     "iopub.status.busy": "2020-12-13T03:27:49.691859Z",
     "iopub.status.idle": "2020-12-13T04:07:01.519985Z",
     "shell.execute_reply": "2020-12-13T04:07:01.521116Z"
    },
    "papermill": {
     "duration": 2351.840337,
     "end_time": "2020-12-13T04:07:01.521775",
     "exception": false,
     "start_time": "2020-12-13T03:27:49.681438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for training and evaluation\n",
    "def train_and_evaluate(train, valid, feature_engineering = False):\n",
    "    \n",
    "    TARGET = 'answered_correctly'\n",
    "    # Features to train and predict\n",
    "    FEATURES = [\n",
    "                'timestamp','content_id','task_container_id','prior_question_elapsed_time',\n",
    "                'part',\n",
    "                #'small_part',\n",
    "                'tags0','tags1','tags2','tags_count',\n",
    "                 \n",
    "                #'tags0_mean',\n",
    "                'contentid_mean',\n",
    "                #'contentid_std',\n",
    "                #'contentid_skew',\n",
    "                #'part_mean',\n",
    "                'last_u_diff_container_id', \n",
    "                'timestamp_u_recency_1', \n",
    "                'timestamp_u_gap_time_ratio', \n",
    "                'timestamp_u_avg_time_ratio', \n",
    "                'timestamp_u_lag_time_ratio', \n",
    "                'timestamp_u_lag_time', \n",
    "\n",
    "                'hist_u_lag_time_raito',\n",
    "                'hist_u_answered_correctly_ratio', \n",
    "                'hist_u_elapsed_time_ratio', \n",
    "                'hist_u_explanation_ratio',\n",
    "                'hist_u_score_ratio', \n",
    "                #'hist_u_same_part_sum',\n",
    "                #'hist_u_same_part_cnt',\n",
    "                'hist_u_same_part_ratio',\n",
    "                #'hist_u_same_content_id_sum',\n",
    "                #'hist_u_same_content_id_cnt',\n",
    "                'hist_u_same_content_id_ratio',\n",
    "                'hist_u_last_incorrect_timestamp',\n",
    "                'hist_u_last_incorrect_cnt',\n",
    "               ]\n",
    "    \n",
    "    gc.collect()\n",
    "    print(f'Traning with {train.shape[0]} rows and {len(FEATURES)} features')    \n",
    "    drop_cols = list(set(train.columns) - set(FEATURES))\n",
    "    y_train = train[TARGET]\n",
    "    y_val = valid[TARGET]\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(drop_cols, axis = 1, inplace = True)\n",
    "    valid.drop(drop_cols, axis = 1, inplace = True)\n",
    "    gc.collect()\n",
    "    \n",
    "    lgb_train = lgb.Dataset(train[FEATURES], y_train)\n",
    "    lgb_valid = lgb.Dataset(valid[FEATURES], y_val)\n",
    "    del train, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    params = {\n",
    "            'objective': 'binary',\n",
    "            'boosting' : 'gbdt',\n",
    "            'metric': 'auc',\n",
    "            'max_depth':-1,\n",
    "            'learning_rate': 0.18,\n",
    "            'num_leaves': 95,#80,\n",
    "            'subsample': 0.83524,#0.8,\n",
    "            'feature_fraction': 0.4,#0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'num_threads':20,\n",
    "            'reg_alpha': 90.312,\n",
    "            'reg_lambda':  7.880,    \n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        feature_name=FEATURES,\n",
    "        categorical_feature=['part','content_id'],\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 200,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose_eval = 50\n",
    "    )\n",
    "    \n",
    "    print('Our Roc Auc score for the validation data is:', roc_auc_score(y_val, model.predict(valid[FEATURES])))\n",
    "    \n",
    "    feature_importance = model.feature_importance()\n",
    "    feature_importance = pd.DataFrame({'Features': FEATURES, 'Importance': feature_importance}).sort_values('Importance', ascending = False)\n",
    "    \n",
    "#     fig = plt.figure(figsize = (10, 10))\n",
    "#     fig.suptitle('Feature Importance', fontsize = 20)\n",
    "#     plt.tick_params(axis = 'x', labelsize = 12)\n",
    "#     plt.tick_params(axis = 'y', labelsize = 12)\n",
    "#     plt.xlabel('Importance', fontsize = 15)\n",
    "#     plt.ylabel('Features', fontsize = 15)\n",
    "#     sns.barplot(x = feature_importance['Importance'], y = feature_importance['Features'], orient = 'h')\n",
    "#     plt.show()\n",
    "    \n",
    "    return TARGET, FEATURES, model\n",
    "\n",
    "# Using time series api that simulates production predictions\n",
    "def inference(TARGET, FEATURES, model, questions_df, prior_question_elapsed_time_mean, features_dicts, global_dicts):\n",
    "    \n",
    "    # Get feature dict\n",
    "    last_u_content_id_dict = features_dicts['last_u_content_id_dict']\n",
    "    last_u_container_id_dict = features_dicts['last_u_container_id_dict']\n",
    "    hist_u_answered_correctly_cnt_dict = features_dicts['hist_u_answered_correctly_cnt_dict']\n",
    "    hist_u_elapsed_time_sum_dict = features_dicts['hist_u_elapsed_time_sum_dict']\n",
    "    hist_u_explanation_sum_dict = features_dicts['hist_u_explanation_sum_dict']\n",
    "    hist_u_same_part_correctly_cnt_dict = features_dicts['hist_u_same_part_correctly_cnt_dict']\n",
    "    hist_u_same_content_id_correctly_cnt_dict = features_dicts['hist_u_same_content_id_correctly_cnt_dict']\n",
    "    timestamp_u = features_dicts['timestamp_u']\n",
    "    hist_u_answered_correctly_sum_dict = features_dicts['hist_u_answered_correctly_sum_dict']\n",
    "    hist_u_score_sum_dict = features_dicts['hist_u_score_sum_dict']\n",
    "    hist_u_same_part_correctly_sum_dict = features_dicts['hist_u_same_part_correctly_sum_dict']\n",
    "    hist_u_same_content_id_correctly_sum_dict = features_dicts['hist_u_same_content_id_correctly_sum_dict']\n",
    "    last_u_last_incorrect_timestamp_dict = features_dicts['last_u_last_incorrect_timestamp_dict']\n",
    "    hist_u_last_incorrect_cnt_dict = features_dicts['hist_u_last_incorrect_cnt_dict']\n",
    "    hist_u_lag_time_sum_dict = features_dicts['hist_u_lag_time_sum_dict']\n",
    "    \n",
    "    # Get global dict\n",
    "    global_avg_q_time_dict = global_dicts['global_avg_q_time_dict']\n",
    "    prior_question_elapsed_time_mean = global_dicts['prior_question_elapsed_time_mean']\n",
    "    global_content_cnt_dict = global_dicts['global_content_cnt_dict']\n",
    "    contentidmean_dict = global_dicts['contentidmean_dict']\n",
    "    \n",
    "    # Get api iterator and predictor\n",
    "    #为了加快训练速度\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    set_predict = env.predict\n",
    "    \n",
    "    previous_test_df = None\n",
    "    for (test_df, sample_prediction_df) in iter_test:\n",
    "        if previous_test_df is not None:\n",
    "            previous_test_df[TARGET] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n",
    "            previous_test_df = pd.merge(previous_test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "            previous_test_df['contentid_mean'] = previous_test_df['content_id'].map(lambda x:contentidmean_dict.get(x,np.nan))\n",
    "            update_features(previous_test_df,\n",
    "                    hist_u_answered_correctly_sum_dict, \n",
    "                    hist_u_score_sum_dict, \n",
    "                    hist_u_same_part_correctly_sum_dict, \n",
    "                    hist_u_same_content_id_correctly_sum_dict,\n",
    "                    last_u_last_incorrect_timestamp_dict,\n",
    "                    hist_u_last_incorrect_cnt_dict)\n",
    "        previous_test_df = test_df.copy()\n",
    "        #测试集处理\n",
    "        test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n",
    "        test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "        test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "        test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "        test_df[TARGET] = 0\n",
    "        test_df['contentid_mean'] = test_df['content_id'].map(lambda x:contentidmean_dict.get(x,np.nan))\n",
    "        test_df = add_features(test_df,\n",
    "        last_u_content_id_dict,\n",
    "        last_u_container_id_dict,\n",
    "        hist_u_answered_correctly_cnt_dict,\n",
    "        hist_u_elapsed_time_sum_dict,\n",
    "        hist_u_explanation_sum_dict,\n",
    "        hist_u_same_part_correctly_cnt_dict,\n",
    "        hist_u_same_content_id_correctly_cnt_dict,\n",
    "        timestamp_u,\n",
    "        hist_u_answered_correctly_sum_dict,\n",
    "        hist_u_score_sum_dict,\n",
    "        hist_u_same_part_correctly_sum_dict,\n",
    "        hist_u_same_content_id_correctly_sum_dict,\n",
    "        last_u_last_incorrect_timestamp_dict,\n",
    "        hist_u_last_incorrect_cnt_dict,\n",
    "        hist_u_lag_time_sum_dict,\n",
    "        global_avg_q_time_dict, prior_question_elapsed_time_mean, global_content_cnt_dict,\n",
    "        update = False)\n",
    "        test_df[TARGET] =  model.predict(test_df[FEATURES])\n",
    "        set_predict(test_df[['row_id', TARGET]])\n",
    "        \n",
    "    print('Job Done')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-13T03:27:49.693474Z",
     "iopub.status.busy": "2020-12-13T03:27:49.691859Z",
     "iopub.status.idle": "2020-12-13T04:07:01.519985Z",
     "shell.execute_reply": "2020-12-13T04:07:01.521116Z"
    },
    "papermill": {
     "duration": 2351.840337,
     "end_time": "2020-12-13T04:07:01.521775",
     "exception": false,
     "start_time": "2020-12-13T03:27:49.681438",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature calculation started...\n",
      "\n",
      "\n",
      "PART 0\n",
      "[(1273157, 20), (1190113, 20), (1293439, 20), (1286013, 20), (1142617, 20), (1284983, 20), (1212260, 20), (1234941, 20), (1273584, 20), (1306545, 20), (1270594, 20), (1330058, 20), (1250183, 20), (1280161, 20), (1275412, 20), (1229201, 20), (1267851, 20), (1325088, 20), (1316436, 20), (1366953, 20)]\n",
      "PART 1\n",
      "[(1164508, 20), (1237867, 20), (1272077, 20), (1341507, 20), (1293892, 20), (1302419, 20), (1281518, 20), (1329729, 20), (1301398, 20), (1308315, 20), (1319474, 20), (1342370, 20), (1245570, 20), (1348126, 20), (1238388, 20), (1194702, 20), (1289286, 20), (1229456, 20), (1236714, 20), (1264369, 20)]\n",
      "PART 2\n",
      "[(1257937, 20), (1293249, 20), (1240743, 20), (1248379, 20), (1271230, 20), (1154834, 20), (1317633, 20), (1235167, 20), (1289058, 20), (1230051, 20), (1295384, 20), (1268853, 20), (1221032, 20), (1299774, 20), (1222735, 20), (1230430, 20), (1289398, 20), (1282153, 20), (1245170, 20), (1230032, 20)]\n",
      "PART 3\n",
      "[(1201364, 20), (1274155, 20), (1268883, 20), (1314932, 20), (1323734, 20), (1347547, 20), (1266170, 20), (1364050, 20), (1251564, 20), (1289869, 20), (1265055, 20), (1274476, 20), (1194603, 20), (1295493, 20), (1101817, 20), (1025457, 20), (1094916, 20), (957175, 20), (1071293, 20), (14231, 20)]\n",
      "User feature calculation completed...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, valid, questions_df, prior_question_elapsed_time_mean, features_dicts, global_dicts = read_and_preprocess(feature_engineering = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/riiid-test-answer-prediction/train.csv', \n",
    "    nrows=None, \n",
    "    dtype={\n",
    "        'row_id': 'int64', \n",
    "        'timestamp': 'int64', \n",
    "        'user_id': 'int32', \n",
    "        'content_id': 'int16', \n",
    "        'content_type_id': 'int8',\n",
    "        'task_container_id': 'int16', \n",
    "        'user_answer': 'int8', \n",
    "        'answered_correctly': 'int8', \n",
    "        'prior_question_elapsed_time': 'float32', \n",
    "        'prior_question_had_explanation': 'boolean'\n",
    "    }\n",
    ")\n",
    "all_user_ids = train_data['user_id'].unique()\n",
    "os.makedirs('/root/hist_caches',exist_ok='ignore')\n",
    "for user_id in tqdm(all_user_ids):\n",
    "    tmp = {\n",
    "        'last_u_content_id_dict':features_dicts['last_u_content_id_dict'][user_id],\n",
    "        'last_u_container_id_dict':features_dicts['last_u_container_id_dict'][user_id],\n",
    "        'hist_u_answered_correctly_cnt_dict':features_dicts['hist_u_answered_correctly_cnt_dict'][user_id],\n",
    "        'hist_u_elapsed_time_sum_dict':features_dicts['hist_u_elapsed_time_sum_dict'][user_id],\n",
    "        'hist_u_explanation_sum_dict':features_dicts['hist_u_explanation_sum_dict'][user_id],\n",
    "        'hist_u_same_part_correctly_cnt_dict':features_dicts['hist_u_same_part_correctly_cnt_dict'][user_id],\n",
    "        'hist_u_same_content_id_correctly_cnt_dict':features_dicts['hist_u_same_content_id_correctly_cnt_dict'][user_id],\n",
    "        'timestamp_u':features_dicts['timestamp_u'][user_id],\n",
    "        'hist_u_answered_correctly_sum_dict':features_dicts['hist_u_answered_correctly_sum_dict'][user_id],\n",
    "        'hist_u_score_sum_dict':features_dicts['hist_u_score_sum_dict'][user_id],\n",
    "        'hist_u_same_part_correctly_sum_dict':features_dicts['hist_u_same_part_correctly_sum_dict'][user_id],\n",
    "        'hist_u_same_content_id_correctly_sum_dict':features_dicts['hist_u_same_content_id_correctly_sum_dict'][user_id],\n",
    "        'last_u_last_incorrect_timestamp_dict':features_dicts['last_u_last_incorrect_timestamp_dict'][user_id],\n",
    "        'hist_u_last_incorrect_cnt_dict':features_dicts['hist_u_last_incorrect_cnt_dict'][user_id],\n",
    "        'hist_u_lag_time_sum_dict':features_dicts['hist_u_lag_time_sum_dict'][user_id],\n",
    "    }\n",
    "    pd.to_pickle(tmp,'/root/hist_caches/{}.pkl'.format(user_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning with 70483829 rows and 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1555: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['content_id', 'part']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 46151803, number of negative: 24332026\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.520485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 15573\n",
      "[LightGBM] [Info] Number of data points in the train set: 70483829, number of used features: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654786 -> initscore=0.640143\n",
      "[LightGBM] [Info] Start training from score 0.640143\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's auc: 0.785248\tvalid_1's auc: 0.784783\n",
      "[100]\ttraining's auc: 0.789197\tvalid_1's auc: 0.788418\n",
      "[150]\ttraining's auc: 0.791246\tvalid_1's auc: 0.79012\n",
      "[200]\ttraining's auc: 0.792532\tvalid_1's auc: 0.791054\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's auc: 0.792532\tvalid_1's auc: 0.791054\n",
      "Our Roc Auc score for the validation data is: 0.7910536980655327\n"
     ]
    }
   ],
   "source": [
    "TARGET, FEATURES, model = train_and_evaluate(train, valid, feature_engineering = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle([TARGET, FEATURES, model, questions_df, prior_question_elapsed_time_mean, features_dicts, global_dicts], SVAE_NAME, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET, FEATURES, model, questions_df, prior_question_elapsed_time_mean, features_dicts, global_dicts = pd.read_pickle('meta.test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2357.989479,
   "end_time": "2020-12-13T04:07:01.778536",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-13T03:27:43.789057",
   "version": "2.1.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
